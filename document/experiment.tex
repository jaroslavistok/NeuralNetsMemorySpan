\chapter{Experiment}
Experimentom v našej práci je meranie hĺbky pamäte a vyhodnotenie vplyvu rôznych
hyperparametrov a typov kontextov na hĺbku pamäte rekurentných SOM.
Cieľom nášho experimentu je aj nájdenie 
optimálnej kombinácie parametrov pre všetky typy porovnávaných sietí a ich vzájomné porovnanie.

\section{Výber konkrétnych trénovacích množín pre experiment}
Trénovacie množiny sme sa snažili zvoliť
takým spôsobom aby sme na nich vedeli otestovať rôzne vlastnosti rekurentných sietí.

Pre náš experiment sme zvolili 3 trénovacie množiny:

\begin{itemize}
    \item \textbf{abcd}\\
    Hlavnou trénovaciou množinou, ktorú používame v našom experimente, je náhodne generovaná 
    sekvencia dlhá 1000 znakov, ktorá pozostáva z písmen $abcd$.
    Táto sekvencia obsahuje dostatočné množstvo regularít a malé množstvo unikátnych znakov a teda
    aj siete s relatívne malým počtom neurónov sa na nej vedia dobre natrénovať.
    Používame ju pri hľadaní optimálnych parametrov pre jednotlivé typy sietí.
    \item \textbf{reber} \\
    Ako druhú trénovaciu množinu sme zvolili sekvenciu dlhú 1000 znakov, pričom znaky sú generované
    špeciálnym pravdepodobnostným stavovým automatom (Reberov automat). Automat generuje znaky z množiny znakov $ptvxse$.
    Táto sekvencia je pre SOMky ťažšia na naučenie a používame ju na overenie toho, či sú siete schopné natrénovať sa aj
    na zložitejších nenáhodných sekvenciách. Pri SRN je použitie tejto trénovacej množiny zaujímavejšie,
    vďaka vlastnostiam, ktoré SRN má.
    \item \textbf{corpus}\\
    Tretí dataset je úryvok z korpusu anglického textu dlhý 1000 znakov, z ktorého sú odstránené špeciálne znaky a medzery.
    Keďže ide o reálny zmysluplný text, nie je to úplne náhodná postupnosť znakov, ale obsahuje určité vzory a opakovania, ktoré by siete mohli vedieť zachytiť
    vo svojej vnútorenej reprezentácii.
    Tento dataset používame čisto iba na overenie, či 
    sú SOMky schopné zachytiť vzory aj \\ v prirodzenom jazyku a teda či sú použiteľné aj pre 
    reálne dáta.
\end{itemize}




\section{Hľadanie optimálnych parametrov sietí}
Na to aby sme mohli porovnať hĺbku pamäte rôznych typov sietí museli sme najskôr
nájsť kombináciu parametrov pri ktorých daný typ siete dosahuje najnižšiu kvantizačnú chybu a 
najvyššie hodnoty pamäťových hĺbok. 

Počas trénovania samoorganizujúcich sa máp môžeme meniť a optimalizovať veľké množstvo parametrov. 

Ako prvé sme museli správne nastaviť veľkosť okolia víťazného neurónu.
Veľkosť okolia by nemala byť počas trénovania konštantná, ale mala by sa po každej epoche zmenšovať.
Vo fáze doľaďovania by mala byť čo najmenšia.
Excitáciu neurónu v určitom kroku trénovania určuje excitačná funkcia. Zvolili sme spojitú
excitačnú funkciu so spojitým gausovským okolím. 
\begin{equation}
    N(i^{*}, i) = \exp^{- \frac{d^{2}_{E}(i^{*}, i)}{\lambda^{2}(t)}}
\end{equation}
Najvyššiu hodnotu má excitačná funkcia pre víťazný neurón, hodnota excitačnej funkcie pre ostatné 
neuróny v sieti závisí od ich euklidovskej vzdialenosti v mriežke neurónov od ich víťaza. Veľmi vzdialené neuróny 
majú takmer nulovú excitáciu a updatujú svoje váhy minimálne.
Dôležitý je parameter $\lambda$, ktorým znižujem veľkosť okolia postupne v jednotlivých epochách.
Najlepšie výsledky (najnižšie hodnoty kvantizačnej chyby) sme dosiahli pri použití nasledujúceho vzťahu pre výpočet hodnoty tohto parametra
v jednotlivých epochách:
\begin{equation}
    \lambda{(t)} = \lambda_{i} \cdot (\lambda_{f} /\ \lambda_{i})^{t /\ t_{max}}
\end{equation}
Kde $\lambda_{f}$ je konštanta, ktorá určuje rýchlosť klesania. 
$\lambda_{i}$ je polovica maximálnej vzdialenosti dvoch neurónov v mape, resp. 
vzdialenosť dvoch neurónov na koncoch diagonály mriežky neurónov.
$t$ je číslo aktuálnej epochy trénovania. Parametrer $t_{max}$ je celkový počet 
epôch trénovania.
\\ \\
Ďalšie parametre:

\begin{itemize}
    \item \textbf{Rýchlosť učenia} \\
    Rovnako ako okolie aj rýchlosť učenia siete by mala počas
procesu trénovania postupne klesať. Na začiatku chceme aby sa váhy menili čo najviac
a ku koncu učenia chceme aby sa doľadovali iba detaily.
Máme na výber 2 možnosti. Postupné znižovanie rýchlosti učenia po každom vstupe, alebo 
postupné znižovanie rýchlosti učenia po jednotlivých epochách, pričom počas každej epochy
je rýchlosť učenia konštantná. 
V naších experimentoch sme dosiahli o niečo lepšie výsledky postupním znižovaním rýchlosti
učenia po jednotlivých epochách. Na začiatku trénovania je veľkosť okolia nastavená na polovicu dĺžky diagonály a počas
trénovania sa postupne znižuje.
Hodnoty rýchlosti učenia máme z intervalu $\langle0, 1\rangle$.
    \item \textbf{Veľkosť posuvného okna} \\
    Vhodnú veľkosť posuvného okna sme určili postupním zvyšovaním jeho veľkosti pokiaľ pamäťová hĺbka stúpala. 
Zaujímavosť, ktorú sme zistili počas experimentovania s veľkosťou pamäťového okna, bolo že 
ak zvolíme príliš veľké posuvné okno, výsledná pamäťová hĺbka môže byť mierne skreslená.
Pri veľkom pamäťovom okne nám môžu neuróny, ktoré majú vo svojom pamäťovom okne uloženú iba 
jednu sekvenciu skreslovať výslednú pamäťovú hĺbku, pretože pamäťová hĺbka takýchto
neurónov je rovná veľkosti posuvného okna. Z tohto dôvodu nie je dobré nastaviť veľkosť pamäťového okna na 
príliš veľkú hodnotu, ale treba zvoliť optimálnu hodnotu.
    \item \textbf{Počet neurónov a počet epôch trénovania} \\
    Rozmery mapy a počet trénovacích epôch sme zvolili na základe vlastností zvolenej trénovacej množiny. 
Tiež sme museli brať do úvahy aj časovú náročnosť trénovania sietí (najmä pri RecSOM).
Potrebovali sme aby sa sieť dokázala správne natrénovať na danej trénovacej množine a zároveň, aby nám experimenty dobehli v rozumnom čase.
Keďže SOMky sa dokážu natrénovať relatívne rýchlo, zvolili sme väčšie rozmery mapy (30x30) a o niečo nižší počet trénovacích epôch (10).
S touto kombináciou sme dosiahli nízke hodnoty kvantizačných chýb a dobré hodnoty pamäťových hĺbok.

Pri určovaní vhodnej veľkosti siete je to vždy kompromis medzi rozlišovaciou schopnosťou jednotlivých vstupov, schopnosťou
zachovať podobné vstupy topologicky čo najbližšie pri sebe a výpočtovou náročnosťou. 
SOMky sa trénujú relatívne rýchlo, preto sme nemuseli použiť veľký počet epôch. Pre naše experimenty sme 
použili trénovanie s 10-timi epochami.
    \item \textbf{Inicializácia váh} \\ 
Váhy aj kontextové váhy sietí sú počas experimentov inicializované náhodnými hodnotami z intervalu $\langle0, 1)$ s rovnomerným rozdelením.
\end{itemize}


\subsection{Parametre pre RecSOM}
Pri RecSOM kontext tvorí vektor aktivít neurónov z predchádzajúceho kroku.
Aktivita neurónu $y$ je určená vzťahom:

\begin{equation}
    y_{i} = \exp{(-d_{i})}
\end{equation}

Neobsahuje žiadny meniteľný parameter. Hodnota $d_{i}$ je súčet vzdialenosti vstupného vektora od váhového vektora a kontextového vektora od 
kontextového vektora. So zmenšujúcou sa vzdialenosťou excitácia neurónu rastie exponenciálne, čo 
znamená, že víťaz a susedné neuróny budú mať najvyššiu excitáciu a vzdialené neuróny budú mať malú excitáciu.
Výpočet kontextu pri RecSOM nevieme ovplyvnovať žiadnym parametrom.

Môžeme však meniť parameter $\alpha$, ktorý sa používa pri samotnom výpočte vzdialenosti
vstupu od váhového vektora a kontextu od kontextového vektora. Tento parameter určuje váhu aktuálneho vstupu a váhu kontextu
vo výslednej vzdialenosti.
Nepotrebujeme extra parameter $\beta$ namiesto $(1 - \alpha)$, pretože matematicky sú týmto spôsobom zahrnuté všetky kombinácie
týchto dvoch parametrov.

\begin{equation}
	d_i = (1 - \alpha) \cdot ||x(t) - w_i||^{2} + \alpha \cdot ||y(t-1) - c_i||^{2} \quad c_{i} \in R^{N}
\end{equation}
V našich experimentoch sme testovali všetky hodnoty parametra $\alpha$ z uzavretého intervalu
$\langle0, 1\rangle$ s krokom $0.01$ (dokopy 100 experimentov).
Veľkosť pamäťového okna sme nastavili na hodnotu $10$.

\subsection{Výsledky pre RecSOM}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     Hodnoty alpha & 0 - 1  (krok: 0.01) \\ 
     \hline
     Rozmer & 30x30  \\
     \hline
     Počet epôch trénovania & 20 \\
     \hline
     Veľkosť posuvného okna & 10  \\
     \hline
    \end{tabular}
    \caption{Trénovacie parametre RecSOM siete}
    \label{table:1}
    \end{table}


    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/r_memory_span}
        \caption{RecSOM hodnoty pamäťových hĺbok}
        \label{recsom_memory_span}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/r_errors}
        \caption{RecSOM hodnoty kvantizačných chýb}
        \label{recsom_errors}
    \end{figure}

\subsection{Analýza výsledkov RecSOM}
Keďže pri RecSOM máme iba jeden parameter, na vizualizáciu pamäťových hĺbok sme použili jednoduchý graf (obr. \ref{recsom_memory_span}) hodnôt pamäťovej hĺbky pre rôzne hodnoty parametra $\alpha$.
Krivka zobrazuje hodnoty pamäťových hĺbok na konci poslednej epochy pre určitú hodnotu parametra $\alpha$.
RecSOM dosiahla najvyššiu hodnotu pamäťovej hĺbky $3.3$ pri hodnote parametra $\alpha = 0.35$.
Na grafe (obr. \ref{recsom_memory_span}) je možno vidieť, že vysoké hodnoty pamäťovej hĺbky sieť dosahuje pri hodnotách $\alpha \in \langle 0.3, 0.4 \rangle$. 
Je to z toho dôvodu, že kontextová zložka výpočtu vzdialenosti má omnoho vyššiu dimenziu (600) ako nekontextová zložka (26) a teda je to číselne vysoká hodnota. 
Tým pádom keď je hodnota parametra $\alpha < 0.5$ dávame väčšiu váhu nekontextovej zložke a akokeby vyvažujeme tento rozdiel v dimenziách.
Nízku pamäťovú hĺbku sieť dosahuje pri hodnotách parametra $\alpha > 0.6$.
Dôvodom nízkej pamäťovej hĺbky pre vysoké hodnoty parametra $\alpha$ je opäť rozdiel v dimenzionalite kontextovej a nekontextovej zložky.
Zvyšovaním hodnoty parametra $\alpha$ ešte viac zvyšujeme tento rozdiel. 
Pri vysokých hodnotách $\alpha$ sa teda sieť neorganizuje správne podľa posledného elementu (táto nekontextová zložka je zanedbaná) a aj keď je kontext organizovaný správne, tak tento posledný element to pokazí a výsledná pamäťová hĺbka je nízka.
Toto je zároveň aj vysvetlenie prečo kvantizačná chyba klesá (obr. \ref{recsom_errors}) so stúpajúcou hodnotou parametra
$\alpha$. Pri vysokých hodnotách $\alpha$ nám kvantizačná chyba už nezachytí nekontextovú časť, keďže tá má jednak malú dimenziu a aj malú váhu.
Nulovú  hodnotu pamäťovej hĺbky RecSOM dosiahla keď je $\alpha = 0$, resp. $\alpha = 1$, čiže keď nepracuje s kontextom (je to obyčajná nerekurentná SOM), alebo s aktuálnymi vstupmi. 
Toto sú extrémne prípady, kedy je sieť deformovaná.
RecSOM dosiahla v našom experimente iba priemerné až podpriemerné výsledky. Sieť dosahuje priemerne nízke hodnoty pamäťových hĺbok, navyše jej trénovanie je pomalé v porovnaní s MSOM pri nízkodimenzionálnych vstupoch, kvôli veľkosti kontextu.


Ukážka receptívneho poľa RecSOM siete poli siete pre parametre s hodnotami $\alpha = 0.35$ po poslednej trénovacej epoche na trénovacej množine \uv{abcd}:

\begin{lstlisting}[basicstyle=\footnotesize]
    ['20''3''bbbcccbbaa1'...........................]
    ['cbdacdccaa1''bdacdccaaa1'........'cccbbaaaba1'...................]
    ['bccbbabaca1'.............................]
    [..............................]
    [.....................'accdacaddd1'.....'acabdcddad1'..]
    [...................'4''baccdacadd1'.........]
    [......'bbcccbbaaa1'............'dcdaaacbaa1'..........]
    [......'aaa3''dcddadbdba1'............'aa2'.........]
    [......'adddbbaabd1'.'cbaadcddbb1'...........'cadddbbaab1''bb2'........]
    [.......'abdcddadbd1''dd2'.....................]
    [..............................]
    [..............................]
    [..............'ddbbaabdbb1'...............]
    [..............................]
    [..............................]
    [..............................]
    [..............................]
    [..............................]
    ['bdbabaabcc1''dbabaabccb1'.......................'aadabdcccc1'....]
    [...'bb2''baabdbbcac1'.....................'cc2''bbcacbbbcc1'..]
    [............................'4'.]
    [..........................'acaadabdcc1''badcbdacdc1''5'.]
    ['4'.............................]
    [..........................'7'...]
    [..........................'aaa3'..'bdcbaccdac1']
    [............................'ccaaaacaca1'.]
    [..............................]
    [.............................'aa2']
    [.......................'ccc3'......]
    [.................'bb2'.'acbaadcddb1''aacbaadcdd1'..'3'......]
\end{lstlisting}

Ukážka receptívneho poľa pre vysokú hodnotu parametra $\alpha = 0.9$.


\begin{lstlisting}[basicstyle=\footnotesize]
    [['51''aaaacacabd1''5''acbbbcccbb1'..........................]
['9''5''ddbbaabdbb1'...........................]
['8''aaacacabdc1'............................]
[...'5''3''abdcddadbd1'........................]
[.....'2'........................]
['3'..............'dddbbaabdb1'..............]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[..............................]
[................'bbcccbbaaa1'.............]
[..............................]
['bbadcbdacd1''abacaadabd1'............................]]
\end{lstlisting}

Políčka s číslami označujú neuróny, ktorých pamäťové okno obsahuje uložené sekvencie (ktoré boli víťazmi pre nejaký vstup), ale nemajú žiadnu 
spoločnú podpostupnosť.
Zvyšné políčka zobrazujú najdlhšiu spoločnú podpostupnosť spoločne s celkovým počtom podpostupností 
v danom pamäťovom okne neurónu. Na receptívnom poli môžeme dobre vidieť ako sieť rozmiestňuje posuvné okná medzi jednotlivé pamäťové okná neurónov.
Konkrétne v tomto prípade bola výsledná hodnota pamäťovej hĺbky siete $3.67$.

Vidíme, že pri vysokej hodnote parametra $\alpha = 0.9$ sa nám väčšina posuvných okien zobrazí na menší počet neurónov a do jednej 
oblasti. V prípade, že je $\alpha = 0.35$ nižšia, posuvné okná sú zobrazené na väčší počet neurónov po celej mape.

Keď sa pozrieme na nejaké konkrétne pamäťové okno ($\alpha = 0.9$), pre ktoré bola dĺžka najdlhšej spoločnej podpostupnosti nulová, 
vidíme príčinu nízkej pamäťovej hĺbky, kazí to posledný 
prvok v pamäťovom okne, ktorý končí na iné písmeno ako zvyšné a kvôli tomu je pamäťová hĺbka okna nulová.
Toto sa deje práve kvôli rozdielu v dimenzionalite kontextovej a nekontextovej časti, kedy má kontextová časť veľkú váhu oproti nekontextovej vo funkcii vzdialenosti a sieť neorganizuje správne podľa posledného vstupu.

\begin{lstlisting}[basicstyle=\footnotesize]
['cacabdcdda', 'ccbbabacaa', 'dcdaaacbaa', 'dcbaccdaca', 'cbaccdacad']
\end{lstlisting}

\subsection{Activity RecSOM}
Pri obyčajnej verzii RecSOM nevieme ovplyvniť žiadnym parametrom výpočet a vlastnosti kontextu. 
Preto sme sa rozhodli vytvoriť si modifikovanú verziu RecSOM. Rozdiel oproti pôvodnej verzii je v spôsobe počítania 
aktivácie neurónov v kontexte. 
Upravili sme pôvodný vzorec 
\begin{equation}
    y_{i} = \exp{(-d_{i})}
\end{equation}
tak aby obsahoval parameter $\beta$.
\begin{equation}
    y_{i} = \exp^{(-\beta \cdot d_{i}^2)}
\end{equation}

Hodnota $d^2$ je umocnená euklidovská vzdialenosť váh neurónu od vstupu.
Na výpočet aktivity neurónu teda používame gaussovskú funkciu, ktorej \uv{strmosť} ovplyvňujeme
pomocou $\beta$ parametra. To znamená, že ovplyvňujeme rozdiely medzi hodnotami aktivácie víťazného neurónu
a susedných neurónov. Táto funkcia je podobná funkcii susednosti, ktorá sa používa počas trénovania. 

Pre malé hodnoty parametra $\beta$ hodnoty aktivácie neurónov so stúpajúcou vzdialenosťou od víťaza
klesajú pomaly. 

Čím je $\beta$ parameter väčší tým je táto funkcia strmšia (väčší skok), čo znamená, že víťaz bude mať výrazne vyššiu hodnotu aktivácie
ako neuróny, ktorých vzdialenosť $d$ od vstupu a kontextu je vyššia.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/plots}
    \caption{Grafy priebehu funkcií na výpočet aktivácií neurónov v Activity RecSOM}
\end{figure}
Na grafe je možno vidno rozdiely v priebehoch tejto funkcie s rôznymi hodnotami parametra $\beta$. Zelenou 
je vyznačený priebeh funkcie aktivácie, ktorý je používaný v pôvodnej RecSOM.

Samotnú hodnotu aktivácie sme chceli ešte normalizovať sumou všetkých aktivácii:
\begin{equation}
    y_{i} = \frac{\exp^{(-\beta \cdot d_{i}^{2})}}{\sum_{j} \exp^{(-\beta \cdot d_{j}^{2})}}
\end{equation}
Pri použití normalizácie sme dostávali signifikantne horšie výsledky
ako bez použitia normalizácie. Dôvodom bolo pravdepodobne to, že vychádzali veľmi malé
hodnoty aktivácií a rozdiely medzi jednotlivými hodnotami boli veľmi malé. Z tohto dôvodu sme zostali 
pri pôvodnej nenormalizovanej verzii.

V prípade ak je hodnota aktivácie normovaná, potom môžeme túto
hodnotu interpretovať aj ako bayesovskú pravdepodobnosť:
Aktivita neurónu vyjadruje bayesovskú pravdepdobnosť, že vstup zodpovedá reprezentácii vo váhach daného neurónu.

\subsection{Activity RecSOM parametre}
V našom experimente sme vyskúšali kombinácie parametrov $\alpha$ a $\beta$.
Hodnoty parametra $\alpha$ sme zvolili z intervalu $\langle0, 1\rangle$ s krokom $0.1$
Hodnoty parametra $\beta$ sme zvolili tak aby sme otestovali rôzne strmosti aktivačnej funkcie.
Konkrétne sme použili tieto hodnoty: $[5.0, 12.0, 13.0, 14.0, 15.0, 20.0, 30.0, 40.0, 50.0, 100.0]$
Pustili sme trénovanie na všetkých kombináciach parametrov $\alpha$ a $\beta$.

\subsection{Výsledky pre Activity RecSOM}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     Hodnoty alpha & 0 - 1 (krok 0.1) \\ 
     \hline
     Hodnoty beta & [5.0, 12.0, 13.0, 14.0, 15.0, 20.0, 30.0, 40.0, 50.0, 100.0]\\ 
     \hline
     Veľkosť & 30x30  \\
     \hline
     počet epôch trénovania & 20  \\
     \hline
     veľkosť posuvného okna & 10  \\
     \hline
    \end{tabular}
    \caption{Parametre Activity RecSOM siete}
    \label{table:1}
    \end{table}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/ar_memory_span}
        \caption{Activity RecSOM hodnoty pamäťových hĺbok}
        \label{memory_span_activity_recsom}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/ar_errors}
        \caption{Activity RecSOM hodnoty kvantizačných chýb}
        \label{errors_activity_recsom}
    \end{figure}

    
\subsection{Analýza výsledkov Activity RecSOM}
Na x-ovej osi sú hodnoty parametra $\alpha$ a na y-ovej osi sú hodnoty parametra $\beta$.
Na grafe pamäťových hĺbok (obr. \ref{memory_span_activity_recsom}) pre Activity RecSOM môžeme vidieť, že sieť dosahuje 
maximálnu pamäťovú hĺbku pri hodnotách parametrov $\alpha = 0.7$ a $\beta = 13.0$.
Najnižšiu pamäťovú hĺbku sieť dosiahla pri hodnote parametrov  $\alpha = 1.0$, čo je špeciálny prípad
kedy funkcia vzdialenosti úplne ignoruje aktuálny vstup a teda sa nevie správne natrénovať. 
V tomto prípade sa rekurentná SOM zredukuje na obyčajnú nerekurentnú SOM. 
Activity RecSOM dosahuje v priemere o niečo nižšie hodnoty pamäťovej hĺbky ako RecSOM.
Na grafe kvantizačných chýb (obr. \ref{errors_activity_recsom}) môžeme vidieť, že sieť sa najlepšie trénuje, keď je hodnota 
parametra $\alpha$ nízka a hodnota parametra $\beta$ vysoká. Čiže keď je vplyv kontextu nízky, tak sa sieť lepšie trénuje.
Keď porovnáme kvantizačnú chybu s obyčajnou RecSOM tak Activity RecSOM má nižšie hodnoty kvantizačnej chyby. 
Našou modifikáciou RecSOM sme síce nedosiahli vyššie hodnoty pamäťovej hĺbky, ale znížili sme kvantizačnú chybu siete.
Zvyšovanie hodnoty $\beta$ parametra teda znižuje kvantizačnú chybu siete, čiže schopnosť siete správne sa natrénovať.
Pri vyšších hodnotách parametra $\beta$ majú neuróny blízke víťazovi výrazne vyššiu hodnotu aktivácie vďaka strmšiemu priebehu funkcie na výpočet aktivácie. 
Inými slovami, informácie uložené v kontexte sú viac sústredené na aktivitu víťazného neurónu. 
Podobne ako pri RecSOM rozdiel medzi dimenziami váhových vektorov (26) a kontextových vektorov (400) je veľký a vzdialenosť medzi kontextom a kontextovým vektorom
je vo výpočte vzdialenosti je kvôli tomu vyššia. Vyššie hodnoty parametra $\beta$ spôsobujú, že aktivita väčšiny neurónov v sieti je blízka nule (aktivita je sústredená na víťazný neurón a zvyšené majú aktivity blízke nule), čo znižuje aj hodnotu
vzdialenosti medzi kontextovými váhami a kontextom a teda klesá aj kvantizačná chyba. Inými slovami nepomer medzi dimenziou kontextu siete a dimenziou vstupu sa zmenšuje.

Dimenzionálny rozdiel medzi vstupom a kontextom je pravdepodobne jednou z príčin toho, že RecSOM a Activity RecSOM dosahujú nižšie hodnoty 
pamäťovej hĺbky ako MSOM a Decay MSOM, ako uvidíme v ďalších experimentoch.


\subsection{MSOM parametre}
Pri MSOM máme okrem $\alpha$ parametra, používaného pri výpočte vzdialenosti, opäť aj $\beta$ parameter, ktorý určuje váhu
váhového vektora víťaza z predchádzajúceho kroku $w_{i^{*}}$ a váhu kontextu
z predchádzajúceho kroku $y_{i^{*}}$ pri výpočte kontextu. Je nazývaný aj ako "zmiešavací" parameter
a určuje váhu jednotlivých zložiek vlastností víťazného neurónu v kontexte.
V našom experimente skúšame všetky kombinácie $\alpha$ a $\beta$ parametrov.
Hodnoty pre oba parametre sú z uzavretého intervalu $\langle 0, 1\rangle$ s krokom $0.1$ (100 experimentov).
Pri experimentovaní s MSOM sa snažíme zistiť aký vplyv má odlišný kontext, ktorý obsahuje iba informáciu
o víťazovi z predchádzajúceho kroku, na pamäťovú hĺbku siete. MSOM má veľkú výhodu v signifikantne 
vyššej rýchlosti učenia, vďaka zredukovanej dimenzie kontextu.

\subsection{Výsledky pre MSOM}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     Hodnoty alpha & 0 - 1 (krok 0.1)  \\ 
     \hline
     Hodnoty beta & 0 - 1  (krok 0.1) \\ 
     \hline
     Veľkosť & 30x30  \\
     \hline
     Počet epôch & 20  \\
     \hline
     Veľkosť posuvného okna & 10  \\
     \hline
    \end{tabular}
    \caption{Parametre MSOM siete}
    \label{table:1}
    \end{table}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/m_memory_span}
        \caption{MSOM hodnoty pamäťových hĺbok}
        \label{msom_memory_span}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/m_errors}
        \caption{MSOM hodnoty kvantizačných chýb}
        \label{msom_errors}
    \end{figure}

 Na x-ovej osi sú hodnoty parametra $\beta$ a na y-ovej osi sú hodnoty parametra $\alpha$. 
 Na grafe (obr. \ref{msom_memory_span}) môžeme vidieť, že, pamäťová hĺbka pri MSOM dosahuje maximálnu 
 hodnotu pamäťovej hĺbky $8.4$ pri hodnote parametra $\alpha = 1$ a $\beta = 0.3$.  
 Minimálne hodnoty pamäťovej hĺbky dosahuje ak je sú hodnoty parametrov $\alpha = 0$ alebo $\beta = 1$. 
 Ak je $\alpha = 0$, tak sa zanedbáva kontextová zložka pri výpočte vzdialenosti a teda sieť nepoužíva žiadny kontext do minulosti. Ide o deformovaný prípad, resp. 
 nejde už o rekurentnú SOM ale o obyčajnú nerekurentnú SOM. Preto je aj kvantizačná chyba v tomto prípade nulová.
 Keď je $\beta = 1$ tak máme prakticky neobmedzený kontext do minulosti, ale kontext je prázdny (nie je v ňom žiadna informácia), preto je 
 pamäťová hĺbka v tomto prípade nulová. Môžeme tvrdiť, že čím je parameter $\beta$ väčší, tým väčší kontext do minulosti máme.
 
 Zaujímavý je prípad, keď je $\alpha = 1$, vtedy sieť pracuje iba s kontextom bez aktuálnych vstupov a dosahuje na jednoduchej trénovacej množine (abcd)
 vysoké hodnoty pamäťových hĺbok.
 Na grafe (obr. \ref{msom_errors}) môžeme vidieť, že pri tejto hodnote parametra má sieť nízku kvantizačnú chybu, čiže sa aj správne učí, ale je kompletne 
 zanedbaná nekontextová časť. Najvyššiu kvantizačnú chybu má MSOM v prípade ak je váha kontextovej zložky vysoká a zároveň sieť nemá žiadny kontext do minulosti($\beta = 0$).

 MSOM dosahuje v našom experimente priemerne vysoké hodnoty pamäťovej hĺbky, pričom je to zároveň 
 výpočtovo najefektívnejšia verzia rekurentnej SOM pri nízko dimenzionálnych vstupoch. 
 Ukazuje sa, že kontext redukovaný na vlastnosti víťaza z predchádzajúceho kroku neovplyvňuje negatívne pamäťovú hĺbku siete na jednoduchej trénovacej množine.

\subsection{Decaying MSOM}
Pre potreby nášho experimentu sme si vytvorili ďalšiu modifikovanú verziu % prida poznamku pod ciarou
rekurentnej SOM. Pri RecSOM kontext tvorí vektor aktivácii všetkých neurónov z predchádzajúceho kroku, 
pri MSOM je to kombinácia vlastností víťazného neurónu z predchádzajúceho kroku. 
Preto sme sa rozhodli použiť odlišný typ kontextu, ktorý bude tvorený kombináciou predchádzajúcich vstupov 
siete a nie stavmi siete z minulých krokov. To znamená, že kontext nie je ovplyvnený samotným procesom trénovania
ani tým, čo sa sieť naučila v predchádzajúcich krokoch, ale iba samotnými vstupnými dátami.
Zvyšné vlastnosti siete zostávajú rovnaké ako v iných rekurentných SOM.

Kontext počítame pomocou nasledujúceho rekurzívneho vzťahu:
\begin{equation} \label{dm_recursive}
    c_{t} = x_{t} + \beta * c_{t-1}
\end{equation}
ktorý v rozvinutej forme môžeme zapísať ako:
\begin{equation}
	c = \beta^{0} \cdot x_{t} + \beta^{1} \cdot x_{t-1} + 
	\beta^{2} \cdot x_{t-2} \ddots \beta^{n} \cdot x_{t-n}
\end{equation}

$\beta$ parameter je číslo z intervalu $\beta \in (0, 1)$ a
$x_t, x_{t-1}, x_{t-2} ...$ sú vstupné vektory z predchádzajúcich krokov.
$t$ je číslo aktuálneho kroku a $n$ je veľkosť trénovacej množiny.

Z rekurzívneho vzťahu vyplýva, že kontext je tvorený kombináciou predchádzajúcich vstupov,
pričom čím dávnejší je vstup, tým menšiu váhu má vo výslednom kontexte, čo je zabezpečené umocňovaním
$\beta$ parametra. Toto sa nazýva leaky integration. V našom prípade
to znamená, že dávne vstupy postupne strácajú na dôležitosti, pričom sa stále sa podieľajú 
na vytváraní výsledného kontextu.

Čím je hodnota parametra $\beta$ vyššia, tým viac informácii z predchádzajúcich vstupov v sebe
kontext obsahuje. Dôležitosť dávnejších vstupov exponenciálne klesá.

\subsection{Decaying MSOM parametre}
V experimente opäť skúšame všetky kombinácie $\alpha$ a $\beta$ parametrov.
Hodnoty pre oba parametre sú z uzavretého intervalu $\langle0, 1\rangle$ s krokom $0.1$.

\subsection{Výsledky pre Decaying MSOM}
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     Hodnoty alpha & 0 - 1 (krok 0.1)  \\ 
     \hline
     Hodnoty beta & 0 - 1  (krok 0.1) \\ 
     \hline
     Veľkosť & 20x20  \\
     \hline
     Počet epôch trénovania & 10  \\
     \hline
     Veľkosť posuvného okna & 15 \\
     \hline
    \end{tabular}
    \caption{Parametre Decaying MSOM siete}
    \label{table:1}
    \end{table}
    
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/dm_memory_span}
        \caption{Decaying MSOM hodnoty pamäťových hĺbok}
        \label{decay_memory_span}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{assets/dm_errors}
        \caption{Decaying MSOM hodnoty kvantizačných chýb}
        \label{decay_errors}
    \end{figure}

    
Na x-ovej osi sú hodnoty parametra $\beta$ a na y-ovej osi sú hodnoty parametra $\alpha$.
Ako môžeme vidieť na grafe (obr. \ref{decay_memory_span}) Decaying MSOM dosahuje veľmi vysoké hodnoty pamäťových hĺbok pri hodnote 
parametra $\beta = 1$ (viď. rovnicu \ref{dm_recursive}). V tomto prípade máme v kontexte uloženú kompletne celú históriu vstupov. Pri MSOM sme v tomto prípade mali síce tiež kompletnú históriu vstupov, ale kontext ako taký bol prázdny.
Pri Decay SOM sa nám kontext nestráca pri $\beta = 1$, preto dosahuje veľmi dobré výsledky.
Čo je dôležité si všimnúť na grafe pamäťových hĺbok je, že pre $\beta = 1$ sú najlepšie $\alpha \in \lbrace 0.5, 0.7 \rbrace$, čiže keď kontextová zložka má mierne vyššiu váhu ako nekontextová zložka vo funkcii vzdialenosti. 
Keď je $\alpha = 0$, tak sieť nepracuje s kontextom, takže nemá žiadnu pamäťovú hĺbku,
v podstate je redukovaná na nerekurentnú SOM. Preto je aj kvantizačná chyba v tomto prípade nulová.
Na grafe (obr. \ref{decay_errors}) môžeme vidieť, že kvantizačná chyba pri hodnote parametra $\beta = 1$ je najvyššia, ale nemá to negatívny 
vplyv na pamäťovú hĺbku Decay MSOM a je to z toho dôvodu, že kontext je nezávislý od stavu (natrénovanosti) kontextových váh a závisí čisto iba od minulých vstupov.
Na to aby Decay MSOM dosiahla vysoké hodnoty pamäťových hĺbok, musí mať natrénované váhové vektory na danej trénovacej množine, pamäťová hĺbka začína stúpať až
po niekoľkých epochách.
Môžeme tvrdiť, že kontext je pri Decay MSOM stále rovnako kvalitný. Čím viac informácii v kontexte máme, tým sú hodnoty pamäťovej hĺbky vyššie.

Keď sa pozrieme na receptívne pole Decay SOM, tak vidíme dôvod:

\begin{lstlisting}[basicstyle=\footnotesize]
    [.....'dacdccaaaa1'.'cdccaaaaca1'.'ccaaaacaca1''caaaacacab1'.
    '2'..'cacabdcdda1''acabdcddad1''cabdcddadb1'..'bdcddadbdb1'..
    cddadbdbab1''ddadbdbaba1'..'dadbdbabaa1'..]
    [..............'acacabdcdd1'......'dcddadbdba1'.......'adbdbabaab1']
    [.........'dccaaaacac1'........'abdcddadbd1'..........'dbdbabaabc1']
    [.......'acdccaaaac1'...'aaaacacabd1'..................]
    ['bbadcbdacd1'.'adcbdacdcc1''aa2'.'bdacdccaaa1'......................
    'bdbabaabcc1'.]
    ['badcbdacdc1'..................'babaabccbb1'......'dbabaabccb1'...]
    [.................'baabccbbab1'............]
    [.................'abccbbabac1'......'abaabccbba1'.....]
    [.................'bccbbabaca1'............]
    [........'aabccbbaba1'.....................]
    [.............'cbbabacaad1'................]
    [..............................]
    ['ccbbabacaa1'..'bbabacaada1'..'bacaadabdc1''abacaadabd1'
    ......................]
    [......'acaadabdcc1'.................'dcddbbbbbd1'...'cddbbbbbdc1'.]
    ['babacaadab1'.................'ddbbbbbdcb1'......'dbbbbbdcba1'....]
    [.......'caadabdccc1'............'bbbdcbaccd1'.....'bbbbbdcbac1'...]
    [..............................]
    ['aadabdcccc1'...................'bdcbaccdac1'...'bbdcbaccda1'.....]
    [.............'bbbbdcbacc1'................]
    ['dabdccccdc1'.'adabdccccd1'...........'dcbaccdaca1''cbaccdacad1'
    'baccdacadd1'.............]
    [..............................]
    ['abdccccdcd1'..........'adcddbbbbb1'..................]
    ['bdccccdcda1'..'ccccdcdaaa1'..'cccdcdaaac1'...........
    'cadddbbaab1'...........]
    ['dccccdcdaa1'.............'cdacadddbb1'...............]
    [..............'dacadddbba1'...............]
    [...'dcdaaacbaa1'............'dbbaabdbbc1'.............]
    [...'cdaaacbaad1''daaacbaadc1'..........'ddbbaabdbb1''bbaabdbbca1'
    'bdbbcacbbb1''10'...........]
    [....'aaacbaadcd1'.......'accdacaddd1'.'acadddbbaa1'...............]
    ['2'............'ccdacadddb1'................]
    [.....'aacbaadcdd1''bb2'.'baadcddbbb1''aadcddbbbb1'....
    '2'.'2''abdbbcacbb1'............]
    
\end{lstlisting}

Posuvné okná sú zobrazované na veľa neurónov po celej sieti, ktorých pamäťové okno obsahuje jednu postupnosť
a tým pádom, je pamäťová hĺbka takejto siete vysoká. Pamäťové okná s nulovou dĺžkou podpostupnosti obsahujú malý počet podpostupností a teda majú aj malú váhu vo výslednej pamäťovej hĺbke.
Kvantizačná chyba je vyššia pretože síce nemáme rozdielne dimenzie kontextovej a nekontextovej časti, 
ale číselne sú vzdialenosti kontextovej časti vyššie hodnoty (kontext je súčet všetkých vstupov pre $\beta = 1$) a tým pádom
kvantizačná chyba vychádza o niečo vyššia, nemá to však vplyv na pamäťovú hĺbku.



Pri predchádzajúcich rekurentných sieťach kontext obsahuje reprezentáciu vplyvov jednotlivých minulých vstupov na sieť a tu je to kombinácia samotných minulých vstupov siete. 
Týmto experimentom sme vyskúšali aký vplyv na pamäťovú hĺbku siete má úplne odlišný druh kontextu a či sme s ním schopný sieť natrénovať.
Naším experimentom sme zistili, že takáto sieť je pri nízkodimenzionálnych vstupoch výpočtovo veľmi efektívna (podobne ako MSOM) a zároveň dosahuje 
najvyššie hodnoty pamäťových hĺbok a spomedzi všetkých testovaných verzií rekurentných SOM.
Kontext tvorený kombináciou minulých vstupov je teda veľmi dobrý.

% vyhodnotenie vysledkov experimentu

% ukazka testov s najlepsimi parametrami

\section {Porovnanie výsledkov SOM}
Po nájdení optimálnych parametrov pre všetky 4 typy sietí, sme spustili 5 behov pre tieto kombinácie parametrov a všetky 3 trénovacie množiny,
pričom sme použili náhodnú inicializáciu váh a spravili priemer týchto hodnôt. Zvyšné hodnoty parametrov sme ponechali
rovnaké ako pri hľadaní optimálnych parametrov.

Výsledky pre hodnoty parametra $\alpha = 0.35$
\subsection{RecSOM}
\begin{itemize}
    \item \textbf{abcd}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{3.12}
        \item Priemerná kvantizačná chyba: \textbf{0.80}
    \end{itemize}
    \item \textbf{reber}
    \item \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{2.476}
        \item Priemerná kvantizačná chyba: \textbf{0.83}
    \end{itemize}
    \item \textbf{corpus}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{0.784}
        \item Priemerná kvantizačná chyba: \textbf{0.54}
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/r_error_top}
    \caption{Graf klesajúcej kvantizačnej chyby počas trénovania RecSOM (abcd)}
    \label{kvantizacna_chyba_recsom}

\end{figure}

Graf (obr. \ref{kvantizacna_chyba_recsom}) zobrazuje vývoj kvantizačnej chyby počas trénovania RecSOM
na trénovacej množine abcd s hodnotou parametra $\alpha = 0.35$.
Z výsledkov vidíme, že so stúpajúcou komplexitou trénovacích množín, klesá pamäťová hĺbka siete.
Na reálnom texte dosiahla sieť len minimálnu hodnotu pamäťovej hĺbky.

\subsection{Activity RecSOM}
Výsledky pre hodnoty parametra $\alpha = 0.7$ a $\beta = 13.0$
\begin{itemize}
    \item \textbf{abcd}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{1.312}
        \item Priemerná kvantizačná chyba: \textbf{0.49}
    \end{itemize}
    \item \textbf{reber}
    \item \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{1,4}
        \item Priemerná kvantizačná chyba: \textbf{0.51}
    \end{itemize}
    \item \textbf{corpus}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{2,49}
        \item Priemerná kvantizačná chyba: \textbf{0.48}
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/ra_error_top}
    \caption{Graf klesajúcej kvantizačnej chyby počas trénovania Activity RecSOM (abcd)}
\end{figure}

Pri Activity RecSOM je to opačne. So stúpajúcou komplexitou trénovacích množín, 
stúpa jej pamäťová hĺbka. Ako sme zistili v experimente s Activity RecSOM, úprava funkcie 
na výpočet aktivity nerónov zlepšila trénovanie siete a preto dokáže dosiahnuť vyššie hodnoty
pamäťových hĺbok.


\subsection{MSOM}
Výsledky pre hodnoty parametra $\alpha = 0.5$ a $\beta = 0.9$
\begin{itemize}
    \item \textbf{abcd}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{7.5}
        \item Priemerná kvantizačná chyba: \textbf{0.04}
    \end{itemize}
    \item \textbf{reber}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{5.72}
        \item Priemerná kvantizačná chyba: \textbf{0.06}
    \end{itemize}
    \item \textbf{corpus}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{1.89}
        \item Priemerná kvantizačná chyba: \textbf{0.08}
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/m_error_top}
    \caption{Graf klesajúcej kvantizačnej chyby počas trénovania MSOM (abcd)}
\end{figure}

Zvolili sme hodnoty parametrov, pri ktorých MSOM dosiahla vysokú pamäťovú hĺbku, ale nie najvyššiu, pretože 
najvyššiu dosiahla pri hodnote $\alpha = 1$, čo je špecifický prípad, kedy sieť nepracuje s nekotextovou zložkou a chceli 
sme sa pri testoch vyhnúť špeciálnym prípadom.
MSOM sa dokáže natrénovať veľmi dobre na jednoduchých trénovacích množinách, ale so stúpajúcou komplexitou trénovacej 
množiny klesá pamäťová hĺbka.
Na reálnom texte však dosahuje nízke hodnoty pamäťovej hĺbky, pravdepodobne 
potrebuje väčšiu veľkosť mapy (viac neurónov), aby sa dokázala správne natrénovať.


\subsection{Decay MSOM}
Výsledky pre hodnoty parametra $\alpha = 0.5$ a $\beta = 1.0$
\begin{itemize}
    \item \textbf{abcd}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{8.44}
        \item Priemerná kvantizačná chyba: \textbf{1.25}
    \end{itemize}
    \item \textbf{reber}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{7.57}
        \item Priemerná kvantizačná chyba: \textbf{1.21}    
    \end{itemize}
    \item \textbf{corpus}
    \begin{itemize}
        \item Priemerná pamäťová hĺbka: \textbf{6.36}
        \item Priemerná kvantizačná chyba: \textbf{1.02}
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/dm_error_top}
    \caption{Graf klesajúcej kvantizačnej chyby počas trénovania Decay MSOM (abcd)}
\end{figure}

Decay MSOM dosahuje stabilne veľmi dobré priemerné hodnoty pamäťových hĺbok na jednoduchých množinách a 
tiež dosahuje najvyššie priemerné hodnoty pamäťových hĺbok na reálnom texte.
V naších experimentoch je Decay MSOM jasný víťaz.

\section{ Vyhodnotenie experimentu so SOM}
Experimentami sme zistili, že hĺbka pamäte je najviac ovplyvnená zložením a dimenziou samotného kontextu rekurentnej SOM.
Parametre, ktoré sme testovali a ktoré vplývajú na hĺbku pamäte rekurentných SOM sú $\alpha$ a $\beta$.
Parameter $\alpha$ vystupuje vo vzťahu pre výpočet vzdialenosti vstupu od určitého neurónu v sieti.
$\beta$ zase ovplyvňuje výpočet samotného kontextu.
Pre nízkodimenzionálne vstupy je najvhodnejšie použiť MSOM, ktorá je výpočtovo najefektívnejšia a dosahuje veľmi dobré 
výsledky.
Pre vysokodimenzionálne vstupy (napríklad bitmapa) je z hľadiska veľkosti kontextu a výpočtovej náročnosti vhodnejšie použiť
Activity RecSOM, ktoré vďaka bohatšiemu kontextu a upravenej funkcie na výpočet aktivácie neurónu vie pracovať aj so zložitejšími trénovacími množinami.


\section{Experiment so SRN a Reberovým automatom}
SRN má niektoré vlastnosti podobné s RecSOM.
V RecSOM máme vrstvu neurónov, ktorá je prepojená s kontextovou vrstvou, podobne 
aj v SRN s Elmanovou architektúrou máme skrytú vrstvu, ktorá je prepojená s kontextovou vrstvou.
Túto analógiu je dobre vidieť ak by sme z Elmanovej siete odstránili výstupnú vrstvu a ponechali iba 
vstupnú, skrytú a kontextovú vrstvu, potom nám zostane architektúra RecSOM siete.
Spôsob trénovania a rozmiestňovania vstupov v priestore je úplne odlišný v prípade Elmanovej siete.

Naším hlavným cieľom pri tomto experimente s SRN bolo preskúmať vlastnosti siete a pokúsiť sa nájsť 
spôsob merania a vyhodnotenia jej pamäťovej hĺbky. 
Tiež sme chceli preskúmať niekoľko zaujímavých vlastností SRN.

Prvá časť experimentu s SRN prebiehala v nasledujúcich krokoch:
\begin{itemize}
    \item Podobne ako pri experimentoch so samoorganizujúcimi sa mapami, ako prvé sme si potrebovali vytvoriť vhodnú trénovaciu množinu.
     Rozhodli sme sa, že použijeme podobné zloženie trénovacej množiny ako pri samoorganizujúcich sa mapách.
     Vstupom je vždy jedno písmeno z náhodne generovanej sekvencie písmen $abcd$ a ako očakávaný výstup je vždy nasledujúce písmeno v sekvencii.
     Takýmto spôsobom sme vytvorili mnnožinu trénovacích príkladov.
     \item Zvolili sme si vhodné aktivačné funkcie:  \\
     Ako aktivačnú funkciu na skrytej vrstve sme použili hyperbolický tangens.
     \begin{equation}
         tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}
     \end{equation}
     Aktivačnú funkciu na výstupnej vrstve sme použili softmax. 
     \item Následne sme museli overiť funkčnosť našej implementácie siete. Ako chybovú funkciu sme použili log loss.
    Počas testovania nám chyba klesala a sieť po natrénovaní predikovala korektné výsledky na testovacích sekvenciách.
     \item Keď sme mali funkčnú implementáciu SRN Elmanovej siete, natrénovali sme ju na našej trénovacej množine.
     Parametre, ktoré sme použili počas trénovania: \\
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|} 
        \hline
        Parameter & Hodnota \\ 
        \hline
        veľkosť skrytej vrstvy & 30  \\
        \hline
        počet epôch & 100  \\
        \hline
        veľkosť posuvného okna na trénovacej množine & 3  \\
        \hline
        počet krokov do minulosti (T) & 5 \\
        \hline
        \end{tabular}
        \caption{Parametre SRN s Elmanovou architektúrou}
        \label{table:1}
    \end{table}
\end{itemize}

SRN si na skrytej vrstve vytvára určitú reprezentáciu vstupov. Preto sme sa rozhodli, že 
správne natrénovanej SRN budeme postupne predkladať písmená z trénovacej množiny, pričom si po každej predikcii, ktorú sieť spraví,
 uložíme aktivácie neurónov (vektor) na skrytej vrstve siete do nejakej množiny. 
Ku každému takémuto vektoru priradíme posuvné okno daného znaku z trénovacej množiny (podobne ako pri experimentoch so SOMkami). 
Po predložení všetkých znakov z trénovacej množiny sieti sme dostali dvojice posuvných okien s prislúchajúcimi aktiváciami neurónov
na skrytej vrstve siete. \\

Tieto dáta sme potrebovali nejakým spôsobom vizualizovať, aby sme v nich vedeli identifikovať prípadné súvislosti medzi reprezentáciou vstupov na skrytej vrstve a 
podobnosťou samotných vstupov.
Rozhodli sme sa, že použijeme vizualizáciu pomocou dendrogramu.
Vizualizácie vo forme dendrogramu sa často používajú ak potrebujeme vizualizovať hierarchické klastrovanie dát.

Na vytvorenie dendrograme potrebujeme vytvoriť tzv. podobnostnú maticu (ang. similarity matrix), kde 
riadky aj stĺpce reprezentujú jednotlivé kontextové vektory a hodnoty v samotnej matici sú euklidovské vzdialenosti medzi týmito vektormi.
Z toho vyplýva, že na diagonále sú samé nulové hodnoty (rovnaké vektory majú medzi sebou nulovú vzdialenosť).

Z takejto matice potom vieme vytvoriť stromový graf, dendrogram, ktorý vizualizuje súvislosti medzi euklidovskou vzdialenosťou jednotlivých vektorov a samotnými posuvnými oknami z trénovacej množiny.
Z toho vieme potom povedať, ktoré postupnosti vstupov sú vo vnútornej reprezentácii SRN blízke. 

Takáto reprezentácia nám hovorí o tom, ako sieť interne reprezentuje dáta z trénovacej množiny.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/dendrogram}
    \caption{Dendrogram pre Elmanovu sieť}
    \label{dendrogram}
\end{figure}
Na x-ovej osi sú posuvné okná a na y-ovej osi sú vzdialenosti medzi jednotlivými vektormi.
Na dendrograme (obr.\ref{dendrogram}) vidíme, že pre vstupy, ktoré sieť vyhodnocuje ako podobné, majú rovnakú euklidovskú vzdialenosť vektorov.
V našom experimente sa v sieti sa vytvorili 4 klustre podobnej veľkosti. Nehovorí nám to bohužiaľ nič o pamäťovej hĺbke Elmanovej siete.
Nevieme ju z takejto vizualizcáie nijakým spôsobom kvantifikovať.

\subsection{Stavový automat na skrytej vrstve}
Zaujímavou vlastnosťou SRN je aj to, že si na skrytej vrstve dokáže vytvoriť vlastnú reprezentáciu stavového automatu, ak 
je trénovaná na trénovacej množine, ktorá je tvorená reťazcom generovaným napríklad reberovým automatom \cite{Servan-Schreiber1991}
Vďaka tejto vlastnosti by sa mala SRN natrénovať na takejto trénovacej množine s nulovou chybou.
Túto vlastnosť sme sa rozhodli overiť.
Vytvorili sme si podobnú trénovaciu množinu ako pri prvom experimente.
Parametre, ktoré sme použili na natrénovanie siete na takejto množine dát:
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
    \hline
    Parameter & Hodnota \\ 
    \hline
    veľkosť skrytej vrstvy & 30  \\
    \hline
    počet epôch & 100  \\
    \hline
    počet krokov do minulosti (T) & 5 \\
    \hline
    \end{tabular}
    \caption{Parametre SRN s Elmanovou architektúrou - reberové reťazce}
    \label{table:1}
\end{table}
Výsledkom tohto trénovania je potvrdenie skutočnosti, že sieť sa dokázala 
natrénovať po 100 epochách s nulovou trénovacou chybou na reťazcoch generovaných reberovým automatom, 
čiže si na svojej skrytej vrstve dokázala vytvoriť reprezentáciu stavového automatu.






