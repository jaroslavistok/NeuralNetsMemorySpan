\chapter{Experiment}

Experimentom v našej práci je určovanie hĺbky pamäte a vyhodnotenie vplyvu rôznych
hyper parametrov a typov kontextov na hĺbku pamäte rekurentných SOM.
Cieľom nášho experimentu je aj nájdenie 
optimálnej kombinácie parametrov pre všetky typy porovnávaných sietí a ich vzájomné porovnanie.

\section{Výber konkrétnych trénovacích množín pre experiment}
Trénovacie množiny sme sa snažili zvoliť
takým spôsobom aby sme na nich vedeli otestovať rôzne vlastnosti rekurentných sietí.

Pre náš experiment sme vybrali 3 trénovacie množiny.
Hlavnou trénovaciou množinou, ktorú používame v našom experimente je náhodne generovaná 
sekvencia dlhá 1000 znakov, ktorá pozostáva z písmen $abcd$.
Táto sekvencia obsahuje dostatočné množstvo regularít a malé množstvo unikátnych znakov a teda
aj siete s relatívne malým počtom neurónov sa na nej vedia dobre natrénovať.
Používame ju pri hľadaní optimálných parametrov pre jednotlivé typy sietí.

Ako druhú trénovaciu množinu sme zvolili sekvenciu dlhú 1000 znakov, pričom znaky sú generované
špeciálnym pravdepodobnostným stavovým automatom (Reberov automat). Automat generuje znaky z množiny znakov $ptvxse$.
Táto sekvencia je pre SOMky ťažšia na naučenie a používame ju na overenie toho, či sú siete schopné natrénovať sa aj
na zložitejších nenáhodných sekvenciách. Pri SRN je použitie tejto trénovacej množiny zaujímavejšie,
vďaka vlastnostiam, ktoré SRN má.

Tretí dataset je úryvok z korpusu anglického textu.
Keďže ide o reálny zmysluplný text, nie je to úplne náhodná postupnosť znakov, ale obsahuje určité vzory a opakovania, ktoré by siete mohli vedieť zachytiť
vo svojej vnútorenej reprezentácii.
Tento dataset používame čisto iba na overenie, či 
sú SOMky schopné zachytiť vzory aj v prirodzenom jazyku a teda či sú použiteľné aj pre 
reálne dáta.

\section{Hľadanie optimálnych parametrov sietí}
Na to aby sme mohli porovnať hĺbku pamäte rôznych typov sietí museli sme najskôr
nájsť kombináciu parametrov pri ktorých daný typ siete dosahuje najnižšiu kvantizačnú chybu a 
najvyššie hodnoty pamäťových hĺbok. 

Pri trénovaní samoorganizujúcich sa máp môžeme meniť a optimalizovať veľké množstvo parametrov. 

Ako prvé sme museli správne nastaviť veľkosť okolia víťazného neurónu.
Veľkosť okolia by nemala byť počas trénovania konštantná, ale mala by sa postupne zmenšovať.
Vo fáze doľaďovania by mala byť čo najmenšia.
Excitáciu neurónu v určitom kroku trénovania určuje excitačná funkcia. Zvolili sme spojitú
excitačnú funkciu so spojitým gausovským okolím. 
\begin{equation}
    N(i^{*}, i) = \exp^{- \frac{d^{2}_{E}(i^{*}, i)}{\lambda^{2}(t)}}
\end{equation}
Najvyššiu hodnotu má excitačná funkcia pre víťazný neurón, hodnota excitačnej funkcie pre ostatné 
neuróny v sieti závisí od ich euklidovskej vzdialenosti v mriežke neurónov od ich víťaza. Veľmi vzdialené neuróny 
majú takmer nulovú excitáciu a updatujú svoje váhy minimálne.
Dôležitý je parameter $\lambda$, ktorým znižujem veľkosť okolia postupne v jednotlivých epochách.
Najlepšie výsledky (najnižšie hodnoty kvantizačnej chyby) sme dosiahli pri použití nasledujúceho vzťahu pre výpočet hodnoty tohto parametra
v jednotlivých epochách:
\begin{equation}
    \lambda{(t)} = \lambda_{i} \cdot (\lambda_{f} /\ \lambda_{i})^{t /\ t_{max}}
\end{equation}
Kde $\lambda_{f}$ je konštanta, ktorá určuje rýchlosť klesania. 
$\lambda_{i}$ je polovica maximálnej vzdialenosti dvoch neurónov v mape, resp. 
vzdialenosť dvoch neurónov na koncoch diagonály mriežky neurónov.
$t$ je číslo aktuálnej epochy trénovania. Parametrer $t_{max}$ je celkový počet 
epôch trénovania.

% rychlost ucenia
Rovnako ako okolie aj rýchlosť učenia siete by mala počas
procesu trénovania postupne klesať. Na začiatku chceme aby sa váhy menili čo najviac
a ku koncu učenia chceme aby sa doľadovali iba detaily.
Máme na výber 2 možnosti. Postupné znižovanie rýchlosti učenia v rámci jednej epochy, alebo 
postupné znižovanie rýchlosti učenia v jednotlivých epochách, pričom počas každej epochy
je rýchlosť učenia konštantná. 
V naších experimentoch sme dosiahli lepšie výsledky postupním 
znižovaním rýchlosti učenia v rámci jednej epochy. 
Hodnoty rýchlosti učenia máme z intervalu $<0, 1>$.

% veľkosť sliding window
Vhodnú veľkosť posuvného okna sme určili postupním zvyšovaním jeho veľkosti pokiaľ pamäťová hĺbka stúpala. 
Zaujímavosť, ktorú sme zistili počas experimentovania s veľkosťou pamäťového okna, bolo že 
ak zvolíme príliš veľké posuvné okno, výsledná pamäťová hĺbka môže byť skreslená.
Pri veľkom pamäťovom okne nám môžu neuróny, ktoré majú vo svojom pamäťovom okne uloženú iba 
jednu sekvenciu skreslovať výslednú pamäťovú hĺbku, pretože pamäťová hĺbka takýchto
neurónov je rovná veľkosti posuvného okna. Z tohto dôvodu nie je dobré nastaviť veľkosť pamäťového okna na 
príliš veľkú hodnotu, ale treba určiť optimálnu hodnotu.

\subsection{Parametre pre RecSOM}
Pri RecSOM kontext tvorí vektor aktivít neurónov z predchádzajúceho kroku.
Aktivita neurónu $y$ je určená vzťahom:

\begin{equation}
    y_{i} = \exp{(-d_{i})}
\end{equation}

Neobsahuje žiadny meniteľný parameter. Hodnota $d_{i}$ je súčet vzdialenosti vstupného vektora od váhového vektora a kontextového vektora od 
kontextového vektora. So zmenšujúcou sa vzdialenosťou excitácia neurónu rastie exponenciálne, čo 
znamená, že víťaz a susedné neuróny budú mať najvyššiu excitáciu a vzdialené neuróny budú mať malú excitáciu.
Výpočet kontextu pri RecSOM nevieme ovplyvnovať žiadnym parametrom.

Môžeme však meniť parameter $\alpha$, ktorý sa používa pri samotnom výpočte vzdialenosti
vstupu od váhového vektora a kontextu od vkontextového vektora. Tento parameter určuje váhu aktuálneho vstupu a váhu kontextu
vo výslednej vzdialenosti.
\begin{equation}
	d_i = (1 - \alpha) \cdot ||x(t) - w_i||^{2} + \alpha \cdot ||y(t-1) - c_i||^{2} \quad c \in R^{N}
\end{equation}
V našich experimentoch sme testovali všetky hodnoty parametra $\alpha$ z uzavretého intervalu
$<0, 1>$ s krokom $0.01$ (dokopy 100 experimentov).

\subsection{Výsledky pre RecSOM}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     alpha & 0 - 1  \\ 
     \hline
     size & 30x30  \\
     \hline
     počet epôch & 10  \\
     \hline
     veľkosť posuvného okna & 30  \\
     \hline
    \end{tabular}
    \caption{Parametre RecSOM siete}
    \label{table:1}
    \end{table}
    

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Pamňťová hĺbka}
        \label{fig:sub1}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Kvantizačná chyba}
        \label{fig:sub2}
    \end{subfigure}
    \caption{A figure with two subfigures}
    \label{fig:test}
\end{figure}
    
% vyhodnotenie vysledkov experimentu

% ukazka testov s najlepsimi parametrami

Výsledky: \\

% výsledky experimentu

\subsection{Activity RecSOM}
Keďže pri obyčajnej verzii RecSOM nevieme ovplyvniť žiadnym parametrom výpočet kontextu. Preto sme sa 
rozhodli vytvoriť si modifikovanú verziu RecSOM. Rozdiel oproti pôvodnej verzii je v spôsobe počítania 
aktivácie neurónov v kontexte. 
Upravili sme pôvodný vzorec % (pridat poznamku pod ciarov)
\begin{equation}
    y_{i} = \exp{(-d_{i})}
\end{equation}
tak aby obsahoval parameter $\beta$.
\begin{equation}
    y_{i} = \exp^{(-\beta \cdot d^2)}
\end{equation}

Hodnota $d^2$ je umocnená euklidovská vzdialenosť neurónu v mriežke od víťazného neurónu.
Na výpočet aktivity neurónu teda používame gaussovskú funkciu, ktorej "strmosť" ovplyvňujeme
pomocou $\beta$ parametra. To znamená, že ovplyvňujeme rozdiely medzi hodnotami aktivácie víťazného neurónu
a susedných neurónov. Je to v podstate excitačná funkcia pre výpočet kontextu.

Pre malé hodnoty parametra $\beta$ hodnoty aktivácie neurónov so stúpajúcou vzdialenosťou od víťaza
klesajú pomaly. Graf pre $\beta = 1$:

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/gaus1}
    \caption{}
\end{figure}

Čím je $\beta$ parameter väčší tým je táto funkcia strmšia, čo znamená, že víťaz bude mať veľkú hodnotu aktivácie
ale vzdialenejšie neuróny ju budú mať takmer nulovú.
Pre $\beta = 30$ graf vyzerá nasledovne:

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/gaus30}
    \caption{Leaky MSOM results}
\end{figure}

Samotnú hodnotu aktivácie sme chceli ešte normalizovať sumou všetkých aktivácii:
\begin{equation}
    y_{i} = \frac{\exp^{(-\beta \cdot d_{i}^{2})}}{\sum_{j} \exp^{(-\beta \cdot d_{j}^{2})}}
\end{equation}
Pri použití normalizácie sme dostávali signifikantne horšie výsledky
ako bez použitia normalizácie. Dôvodom bolo pravdepodobne to, že vychádzali veľmi malé
hodnoty aktivácií a rozdiely boli takmer veľmi malé. Z tohto dôvodu sme zostali 
pri pôvodnej nenormalizovanej verzii.

\subsection{Activity RecSOM parametre}
V našom experimente sme vyskúšali kombinácie parametrov $\alpha$ a $\beta$.
Hodnoty parametra $\alpha$ sme zvolili z intervalu $<0, 1>$ s krokom $0.1$
Hodnoty parametra $\beta$ sme zvolili tak aby sme otestovali rôzne strmosti aktivačnej funkcie.
Konkrétne sme použili tieto hodnoty: $[5.0, 12.0, 13.0, 14.0, 15.0, 20.0, 30.0, 40.0, 50.0, 100.0]$
Pustili sme trénovanie na všetkých kombináciach parametrov $\alpha$ a $\beta$.

\subsection{Výsledky pre Activity RecSOM}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     alpha & 0 - 1  \\ 
     \hline
     size & 30x30  \\
     \hline
     počet epôch & 10  \\
     \hline
     veľkosť posuvného okna & 30  \\
     \hline
    \end{tabular}
    \caption{Parametre RecSOM siete}
    \label{table:1}
    \end{table}
    

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Pamňťová hĺbka}
        \label{fig:sub1}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Kvantizačná chyba}
        \label{fig:sub2}
    \end{subfigure}
    \caption{A figure with two subfigures}
    \label{fig:test}
\end{figure}
    
% vyhodnotenie vysledkov experimentu

% ukazka testov s najlepsimi parametrami

\subsection{MSOM parametre}
Pri mSOM máme okrem $\alpha$ parametra aj $\beta$ parameter, ktorý určuje váhu
váhového vektora víťaza z predchádzajúceho kroku $w_{i^{*}}$ a váhu kontextu
z predchádzajúceho kroku $y_{i^{*}}$ pri výpočte kontextu. Je nazývaný aj ako "zmiešavací" parameter
a určuje váhu jednotlivých zložiek kontextu pri výpočte.
V našom experimente skúšame všetky kombinácie $\alpha$ a $\beta$ parametrov.
Hodnoty pre oba parametre sú z uzavretého intervalu $<0, 1>$ s krokom $0.1$ (100 experimentov).
Pri experimentovaní s mSOM sa snažíme zistiť aký vplyv má odlišný kontext, ktorý obsahuje iba informáciu
o víťazovi z predchádzajúceho kroku, na pamäťovú hĺbku siete. mSOM má veľkú výhodu v signifikantne 
vyššej rýchlosti učenia, vďaka zredukovanému kontextu.

\subsection{Výsledky pre mSOM}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     alpha & 0 - 1  \\ 
     \hline
     size & 30x30  \\
     \hline
     počet epôch & 10  \\
     \hline
     veľkosť posuvného okna & 30  \\
     \hline
    \end{tabular}
    \caption{Parametre RecSOM siete}
    \label{table:1}
    \end{table}
    

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Pamňťová hĺbka}
        \label{fig:sub1}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Kvantizačná chyba}
        \label{fig:sub2}
    \end{subfigure}
    \caption{A figure with two subfigures}
    \label{fig:test}
\end{figure}
    
% vyhodnotenie vysledkov experimentu

% ukazka testov s najlepsimi parametrami

\subsection{Decaying mSOM}
Pre potreby nášho experimentu sme si vytvorili ďaľšiu modifikovanú verziu % prida poznamku pod ciarou
rekurentnej SOM. Pri RecSOM kontext tvorí vektor aktivácii všetkých neurónov z predchádzajúceho kroku, 
pri mSOM je to kombinácia vlastností víťazného neurónu z predchádzajúceho kroku. 
Preto sme sa rozhodli použiť odlišný typ kontextu, ktorý bude tvorený kombináciou predchádzajúcih vstupov 
siete a nie stavmi siete z minulých krokov.
Zvyšné vlastnosti siete zostávajú rovnaké ako v iných rekurentných SOM.

Kontext počítame pomocou nasledujúceho rekurzívneho vzťahu:
\begin{equation}
	c = \beta^{0} \cdot x_{t} + \beta^{1} \cdot x_{t-1} + 
	\beta^{2} \cdot x_{t-2} \ddots \beta^{n} \cdot x_{t-n}
\end{equation}

$\beta$ parameter je číslo z intervalu $\beta < 1 \wedge \beta > 0$ a
$x_t, x_{t-1}, x_{t-2} ...$ sú vstupné vektory z predchádzajúcich krokov.
$t$ je číslo aktuálneho kroku a $n$ je veľkosť trénovacej množiny.

Z rekurzívneho vzťahu vyplýva, že kontext je tvorený kombináciou predchádzajúcich vstupov
pričom čím dávnejší je vstup, tým menšiu váhu má vo výslednom kontexte, čo je zabezpečené umocňovaním
$\beta$ parametra. Toto sa nazýva leaky integration. V našom prípade
to znamená, že dávne vstupy postupne strácajú na dôležitosti pričom sa stále sa podieľajú 
na vytváraní výsledného kontextu.

Čím je hodnota parametra $\beta$ vyššia, tým viac informácii z predchádzajúcich vstupov v sebe
kontext obsahuje. Dôležitosť dávnejších vstupov exponenciálne klesá.

\subsection{Decaying MSOM}

V experimente opäť skúšame všetky kombinácie $\alpha$ a $\beta$ parametrov.
Hodnoty pre oba parametre sú z uzavretého intervalu $<0, 1>$ s krokom $0.1$

\subsection{Výsledky pre Decaying msom}
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     alpha & 0 - 1  \\ 
     \hline
     size & 30x30  \\
     \hline
     počet epôch & 10  \\
     \hline
     veľkosť posuvného okna & 30  \\
     \hline
    \end{tabular}
    \caption{Parametre RecSOM siete}
    \label{table:1}
    \end{table}
    

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Pamňťová hĺbka}
        \label{fig:sub1}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/recsom_abcd}
        \caption{Kvantizačná chyba}
        \label{fig:sub2}
    \end{subfigure}
    \caption{A figure with two subfigures}
    \label{fig:test}
\end{figure}
    
% vyhodnotenie vysledkov experimentu

% ukazka testov s najlepsimi parametrami


Experimentami sme zistili, že na hĺbku pamäte siete majú vplyv iba niektoré z nich. 
Najdôležitejšie parameters, ktoré vplývajú na hĺbku pamäte neurónovej siete sú parameters $\alpha$ a $\beta$
vo vzťahu pre výpočet vzdialenosti vstupného vektora od určitého neurónu v siete (čiže od jeho váhového a kontextového vektora).
% TODO pridat rovnicu na ilustraciu
Tieto dva parameters určujú pomer dôležitosti aktuálneho vstupu a dôležitosť kontextu, pri výpočte vzdialenosti (kvantizačnej chyby).

Experiment prebiehal nasledujúcim spôsobom:
\begin{itemize}
    \item Vybrali sme vhodnú trénovaciu sekvenciu, počet epôch trénovania a dostatočnú veľkosť pamäťového okna
    \item Spustili sme trénovanie na všetkých kombináciach týchto dvoch parametrov s krokom 0.1
    \item Hodnoty pamäťovej hĺbky sme ukladali do súboru
    \item Na záver sme vykreslili heatmapu, ktorá znázorňuje aká bola pamäťová hĺbka pre rôzne kombinácie parametrov.
\end{itemize}

% TODO vyber rychlosti ucenia (konstantna / postupne zmensujuca?)

Počet epôch sme určili na základe kvantizačnej chyby.
Počet epôch sme postupne zvyšovali a keď kvantizačná chyba prestala signifikantne klesať, resp. dosiahla 
svoje minimum zastavali sme ho na tejto hodnote a ďalej nezvyšovali. SOM sa dokážu relatívne rýchlo učiť a 
teda počet epôch nemusí byť vysoký, čo je veľkou výhodou pri experimentovaní, kedže trénovanie netrvá príliš dlhú dobu
a tým pádom sme mohli vyskúšať viac kombinácii a modifikácii.

Dostatočnú veľkosť pamäťového okna sme určili podobne ako počet epôch. Parameter sme postupne zvyšovali
a zastavili na hodnote, keď pamäťová hĺbka siete prestala stúpať, čiže veľkosť pamäťového okna už
neovplyvňovala hĺbku pamäte a ďalšie zvyšovanie parametra nemalo zmysel. 

Na základe tohto sme zistili, že pre každý typ siete sú ideálne hodnoty týchto parametrov odlišné.


\section{Vyhodnotenie rôznych kombinácii alpha a beta parametrov}

The table \ref{table:1} is an example of referenced \LaTeX elements.
 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|} 
 \hline
 Parameter & Hodnota \\ 
 \hline\hline
 1 & 6  \\ 
 \hline
 2 & 7   \\
 \hline
 3 & 545  \\
 \hline
 4 & 545  \\
 \hline
 5 & 88 \\  
 \hline
\end{tabular}
\caption{Parametre RecSOM siete}
\label{table:1}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/recsom_abcd}
    \caption{Rec SOM results}
\end{figure}


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     1 & 6  \\ 
     \hline
     2 & 7   \\
     \hline
     3 & 545  \\
     \hline
     4 & 545  \\
     \hline
     5 & 88 \\  
     \hline
    \end{tabular}
    \caption{Parametre mSOM siete}
    \label{table:2}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/msom_abcd}
    \caption{MSOM results}
\end{figure}


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|} 
     \hline
     Parameter & Hodnota \\ 
     \hline\hline
     1 & 6  \\ 
     \hline
     2 & 7   \\
     \hline
     3 & 545  \\
     \hline
     4 & 545  \\
     \hline
     5 & 88 \\  
     \hline
    \end{tabular}
    \caption{Parametre leaky mSOM siete}
    \label{table:2}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{assets/leakymsom_abcd}
    \caption{Leaky MSOM results}
\end{figure}


\section{Experiment so SRN a Reberovým automatom}
Pri SRN sme sa pokúšali nájsť spôsob merania pamäťove hĺbky.




